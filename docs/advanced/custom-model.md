---
title: Custom Model
---

# Custom Model

You can use a custom LLM that isn't built into Monacopilot by setting up a `model` when you create a new CompletionCopilot. This feature lets you connect to LLMs from other services or your own custom-built models.

## Example

```javascript
const copilot = new CompletionCopilot(process.env.HUGGINGFACE_API_KEY, {
    // You don't need to set the provider if you are using a custom model.
    model: {
        config: (apiKey, prompt) => ({
            endpoint:
                'https://api-inference.huggingface.co/models/openai-community/gpt2',
            headers: {
                Authorization: `Bearer ${apiKey}`,
                'Content-Type': 'application/json',
            },
            body: {
                inputs: `${prompt.context}\n\n${prompt.instruction}\n\n${prompt.fileContent}`,
                parameters: {
                    max_length: 100,
                    num_return_sequences: 1,
                    temperature: 0.7,
                },
            },
        }),
        transformResponse: response => ({text: response[0].generated_text}),
    },
});
```

## Configuration

The `model` option accepts an object with two functions:

| Function            | Description                                                                                                             | Type                                                                                            |
| ------------------- | ----------------------------------------------------------------------------------------------------------------------- | ----------------------------------------------------------------------------------------------- |
| `config`            | A function that receives the API key and prompt data, and returns the configuration for the custom model API request.   | `(apiKey: string, prompt: PromptData) => { endpoint: string; body?: object; headers?: object }` |
| `transformResponse` | A function that takes the raw/parsed response from the custom model API and returns an object with the `text` property. | `(response: unknown) => { text: string \| null; }`                                              |

### Prompt Data Structure

The `prompt` parameter passed to the `config` function has the following structure:

```typescript
interface PromptData {
    /**
     * Contextual information about the code environment
     * @example filename, technologies, etc.
     */
    context: string;

    /**
     * Instructions for the AI model on how to generate the completion
     */
    instruction: string;

    /**
     * The content of the file being edited
     */
    fileContent: string;
}
```

### Config Return Value

The `config` function must return an object with the following properties:

| Property   | Type                    | Description                                  |
| ---------- | ----------------------- | -------------------------------------------- |
| `endpoint` | `string`                | The URL of the custom model API endpoint.    |
| `body`     | `object` or `undefined` | The body of the custom model API request.    |
| `headers`  | `object` or `undefined` | The headers of the custom model API request. |

### Response Transformation

The `transformResponse` function must return an object with the `text` property. This `text` property should contain the text generated by the custom model. If no valid text can be extracted, the function should return `null` for the `text` property.

## Working with Different Models

When working with different models, you'll need to format the prompt data appropriately for your specific model. For example:

- For models that expect a single string input, you might concatenate the prompt fields:

    ```javascript
    body: {
        inputs: `Context: ${prompt.context}\nInstructions: ${prompt.instruction}\nFile: ${prompt.fileContent}`,
        // other parameters...
    }
    ```

- For models that accept structured input:
    ```javascript
    body: {
        messages: [
            { role: "system", content: prompt.context },
            { role: "user", content: `${prompt.instruction}\n\n${prompt.fileContent}` }
        ],
        // other parameters...
    }
    ```

::: tip
Please ensure you are using a high-quality model, especially for coding tasks, to get the best and most accurate completions. The example above shows how to integrate with Hugging Face's GPT-2, but it's important to note that GPT-2 is not recommended for code completion and is shown only as an implementation example. For production use, choose specialized code-optimized models. Also, use a model with very low response latency (preferably under 1 seconds) to enjoy a great experience and utilize the full power of Monacopilot.
:::
